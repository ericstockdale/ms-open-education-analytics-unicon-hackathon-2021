{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "syn-oea-eshthon21"
		},
		"syn-oea-eshthon21-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'syn-oea-eshthon21-WorkspaceDefaultSqlServer'"
		},
		"syn-oea-eshthon21-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://stoeaeshthon21.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-eshthon21-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('syn-oea-eshthon21-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-eshthon21-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('syn-oea-eshthon21-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ContosoISD_example')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "319a14bc-3fac-4453-b745-92b65416ff13"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/88bc4ad2-b92a-4cec-9c63-bac0d9696a30/resourceGroups/rg-oea-eshthon21/providers/Microsoft.Synapse/workspaces/syn-oea-eshthon21/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://syn-oea-eshthon21.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ContosoISD Example\r\n",
							"This example demonstrates how to use the OEA framework and modules to process incoming data, perform data prep, and view the data in an example Power BI dashboard.\r\n",
							"\r\n",
							"# Running the example\r\n",
							"1) Select your spark pool in the \"Attach to\" dropdown list above.\r\n",
							"\r\n",
							"2) Click on \"Publish\" in the top nav bar (and wait a few seconds for the notification that says \"Publishing completed\").\r\n",
							"\r\n",
							"3) Click on \"Run all\" at the top of this tab (and wait for the processing to complete - which can take around 5 to 10 minutes).\r\n",
							"\r\n",
							"4) Open the dashboard in Power BI desktop and point it to your newly setup data lake (you can download the pbix from here: [techInequityDashboardContoso v2.pbix](https://github.com/microsoft/OpenEduAnalytics/blob/main/packages/ContosoISD/power_bi/techInequityDashboardContoso%20v2.pbix) )\r\n",
							"\r\n",
							"# More info\r\n",
							"See [OEA Solution Guide](https://github.com/microsoft/OpenEduAnalytics/blob/main/docs/OpenEduAnalyticsSolutionGuide.pdf) for more details on this example."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /OEA_py"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /example_modules_py"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 0) Initialize the OEA framework and modules needed.\r\n",
							"oea = OEA()\r\n",
							"m365 = M365(oea)\r\n",
							"contoso_sis = ContosoSIS(oea, 'contoso_sis', False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 1) Land data into stage1 of your data lake, from multiple source systems (this example copies in test data sets that came with the OEA installation).\r\n",
							"contoso_sis.copy_test_data_to_stage1()\r\n",
							"m365.copy_test_data_to_stage1()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 2) Process the raw data (csv format) from stage1 into stage2 (adds schema details and writes out in parquet format).\r\n",
							"#    [Note: we're not performing pseudonymization in this example, so everything is written to container stage2np.]\r\n",
							"m365.process_roster_data_from_stage1()\r\n",
							"contoso_sis.process_data_from_stage1()\r\n",
							"m365.process_activity_data_from_stage1()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 3) Run additional prep on the data to create a unified dataset that can be used in a Power BI report\r\n",
							"\r\n",
							"# Process sectionmark data. Convert id values to use the Person.Id and Section.Id values set in the m365 data.\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentsectionmark'), 'SectionMark')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Person'), 'Person')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Section'), 'Section')\r\n",
							"df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\r\n",
							"sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\r\n",
							"sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\r\n",
							"from SectionMark sm, Person p, Section s \\\r\n",
							"where sm.student_id = p.ExternalId \\\r\n",
							"and sm.section_id = s.ExternalId\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/SectionMark')\r\n",
							"\r\n",
							"# Repeat the above process, this time for student attendance\r\n",
							"# Convert id values to use the Person.Id, Org.Id and Section.Id values\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentattendance'), 'Attendance')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Org'), 'Org')\r\n",
							"df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, to_date(att.attendance_date,'MM/dd/yyyy') AttendanceDate, \\\r\n",
							"att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\r\n",
							"att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\r\n",
							"from Attendance att, Org o, Person p, Section s \\\r\n",
							"where att.student_id = p.ExternalId \\\r\n",
							"and att.school_id = o.ExternalId \\\r\n",
							"and att.section_id = s.ExternalId\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np +'/ContosoISD/Attendance')\r\n",
							"\r\n",
							"# Repeat the above process, this time for student demographics\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentdemographics'), 'Demographics')\r\n",
							"\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Person'), 'Person')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/StudentSectionMembership'), 'StudentSectionMembership')\r\n",
							"\r\n",
							"df = spark.sql(\"select p.Id PersonId, dem.FederalRaceCategory, dem.PrimaryLanguage, dem.ELLStatus, dem.SpecialEducation, dem.LowIncome \\\r\n",
							"from Demographics dem, Person p \\\r\n",
							"where dem.SIS_ID = p.ExternalId\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np +'/ContosoISD/Demographics')\r\n",
							"\r\n",
							"# Add 'Department' column to Course (hardcoded to \"Math\" for this Contoso example)\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Course'), 'Course')\r\n",
							"df = spark.sql(\"select Id, Name, Code, Description, ExternalId, CreateDate, LastModifiedDate, IsActive, CalendarId, 'Math' Department from Course\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/Course')\r\n",
							"\r\n",
							"\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/ContosoISD/SectionMark'), 'PersonSectionMark')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/ContosoISD/Attendance'), 'PersonAttendance')\r\n",
							"\r\n",
							"df = spark.sql(\"select DISTINCT s.CourseId, s.Id SectionId, ssm.PersonId PersonId, \\\r\n",
							"CASE WHEN att.AttendanceCode = 'P' THEN 1 ELSE 0 END As WasPresent, \\\r\n",
							"sm.NumericGrade \\\r\n",
							"from StudentSectionMembership ssm \\\r\n",
							"LEFT JOIN PersonSectionMark sm on sm.SectionId = ssm.SectionId and sm.PersonId = ssm.PersonId \\\r\n",
							"LEFT JOIN PersonAttendance att on att.SectionId = ssm.SectionId and att.PersonId = ssm.PersonId \\\r\n",
							"JOIN Section s on s.Id = ssm.SectionId\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/studentsectionperformance')\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 4) Create spark db's that point to the data in the data lake to allow for connecting via Power BI through use of the Serverless SQL endpoint.\r\n",
							"contoso_sis.create_stage2_db('PARQUET')\r\n",
							"m365.create_stage2_db('PARQUET')\r\n",
							"\r\n",
							"spark.sql('CREATE DATABASE IF NOT EXISTS s2_ContosoISD')\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Activity using PARQUET location '\" + oea.stage2np + \"/m365/TechActivity'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Calendar using PARQUET location '\" + oea.stage2np + \"/m365/Calendar'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Org using PARQUET location '\" + oea.stage2np + \"/m365/Org'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Person using PARQUET location '\" + oea.stage2np + \"/m365/Person'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.PersonIdentifier using PARQUET location '\" + oea.stage2np + \"/m365/PersonIdentifier'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.RefDefinition using PARQUET location '\" + oea.stage2np + \"/m365/RefDefinition'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Section using PARQUET location '\" + oea.stage2np + \"/m365/Section'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Session using PARQUET location '\" + oea.stage2np + \"/m365/Session'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StaffOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StaffOrgAffiliation'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StaffSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StaffSectionMembership'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StudentOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StudentOrgAffiliation'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StudentSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StudentSectionMembership'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Course using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Course'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Attendance using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Attendance'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.SectionMark using PARQUET location '\" + oea.stage2np + \"/ContosoISD/SectionMark'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Demographics using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Demographics'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StudentSectionPerformance using PARQUET location '\" + oea.stage2np + \"/ContosoISD/studentsectionperformance'\")\r\n",
							"\r\n",
							"print(f\"Created spark db's.\\nYou can now open the 'techInequityDashboardContoso v2.pbix' dashboard and change the datasource to point to: {oea.serverless_sql_endpoint}\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything\r\n",
							"You can uncomment line 11 in the cell below and run the cell to reset everything and walk through the process again from the top.\r\n",
							"\r\n",
							"Note: remember to comment out line 11 again to prevent accidental resetting of the example"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def reset_all_processing():\r\n",
							"    contoso_sis.delete_all_stages()\r\n",
							"    m365.delete_all_stages()\r\n",
							"    oea.rm_if_exists(oea.stage2np + '/ContosoISD')\r\n",
							"\r\n",
							"    oea.drop_db('s2_contoso_sis')\r\n",
							"    oea.drop_db('s2_contosoisd')\r\n",
							"    oea.drop_db('s2_m365')\r\n",
							"\r\n",
							"# Uncomment the following line and run this cell to reset everything if you want to walk through the process again.\r\n",
							"reset_all_processing()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 58
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Contoso_ELT')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1bd3cacc-1ec1-479a-98cb-2ad0d12323b7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/88bc4ad2-b92a-4cec-9c63-bac0d9696a30/resourceGroups/rg-oea-eshthon21/providers/Microsoft.Synapse/workspaces/syn-oea-eshthon21/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://syn-oea-eshthon21.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /contosa_module_py"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /m365_module_py"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 0) Initialize the OEA framework and modules needed.\r\n",
							"oea = OEA()\r\n",
							"m365 = M365(oea)\r\n",
							"contoso_sis = ContosoSIS(oea, 'contoso_sis', False)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def reset_all_processing():\r\n",
							"    contoso_sis.delete_all_stages()\r\n",
							"    m365.delete_all_stages()\r\n",
							"    oea.rm_if_exists(oea.stage2np + '/ContosoISD')\r\n",
							"\r\n",
							"    oea.drop_db('s2_contoso_sis')\r\n",
							"    oea.drop_db('s2_contosoisd')\r\n",
							"    oea.drop_db('s2_m365')\r\n",
							"\r\n",
							"# Uncomment the following line and run this cell to reset everything if you want to walk through the process again.\r\n",
							"reset_all_processing()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1) Land data into stage1 of your data lake, from multiple source systems (this example copies in test data sets that came with the OEA installation).\r\n",
							"contoso_sis.copy_test_data_to_stage1()\r\n",
							"m365.copy_test_data_to_stage1()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 2) Process the raw data (csv format) from stage1 into stage2 (adds schema details and writes out in parquet format).\r\n",
							"#    [Note: we're not performing pseudonymization in this example, so everything is written to container stage2np.]\r\n",
							"m365.process_roster_data_from_stage1()\r\n",
							"contoso_sis.process_data_from_stage1()\r\n",
							"m365.process_activity_data_from_stage1()"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 3) Run additional prep on the data to create a unified dataset that can be used in a Power BI report\r\n",
							"\r\n",
							"# Process sectionmark data. Convert id values to use the Person.Id and Section.Id values set in the m365 data.\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentsectionmark'), 'SectionMark')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Person'), 'Person')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Section'), 'Section')\r\n",
							"df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\r\n",
							"sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\r\n",
							"sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\r\n",
							"from SectionMark sm, Person p, Section s \\\r\n",
							"where sm.student_id = p.ExternalId \\\r\n",
							"and sm.section_id = s.ExternalId\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/SectionMark')\r\n",
							"\r\n",
							"# Repeat the above process, this time for student attendance\r\n",
							"# Convert id values to use the Person.Id, Org.Id and Section.Id values\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentattendance'), 'Attendance')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Org'), 'Org')\r\n",
							"df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, to_date(att.attendance_date,'MM/dd/yyyy') AttendanceDate, \\\r\n",
							"att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\r\n",
							"att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\r\n",
							"from Attendance att, Org o, Person p, Section s \\\r\n",
							"where att.student_id = p.ExternalId \\\r\n",
							"and att.school_id = o.ExternalId \\\r\n",
							"and att.section_id = s.ExternalId\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np +'/ContosoISD/Attendance')\r\n",
							"\r\n",
							"# Repeat the above process, this time for student demographics\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentdemographics'), 'Demographics')\r\n",
							"\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Person'), 'Person')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/StudentSectionMembership'), 'StudentSectionMembership')\r\n",
							"\r\n",
							"df = spark.sql(\"select p.Id PersonId, dem.FederalRaceCategory, dem.PrimaryLanguage, dem.ELLStatus, dem.SpecialEducation, dem.LowIncome \\\r\n",
							"from Demographics dem, Person p \\\r\n",
							"where dem.SIS_ID = p.ExternalId\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np +'/ContosoISD/Demographics')\r\n",
							"\r\n",
							"# Add 'Department' column to Course (hardcoded to \"Math\" for this Contoso example)\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Course'), 'Course')\r\n",
							"df = spark.sql(\"select Id, Name, Code, Description, ExternalId, CreateDate, LastModifiedDate, IsActive, CalendarId, 'Math' Department from Course\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/Course')\r\n",
							"\r\n",
							"\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/ContosoISD/SectionMark'), 'PersonSectionMark')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/ContosoISD/Attendance'), 'PersonAttendance')\r\n",
							"\r\n",
							"df = spark.sql(\"select DISTINCT s.CourseId, s.Id SectionId, ssm.PersonId PersonId, \\\r\n",
							"CASE WHEN att.AttendanceCode = 'P' THEN 1 ELSE 0 END As WasPresent, \\\r\n",
							"sm.NumericGrade \\\r\n",
							"from StudentSectionMembership ssm \\\r\n",
							"LEFT JOIN PersonSectionMark sm on sm.SectionId = ssm.SectionId and sm.PersonId = ssm.PersonId \\\r\n",
							"LEFT JOIN PersonAttendance att on att.SectionId = ssm.SectionId and att.PersonId = ssm.PersonId \\\r\n",
							"JOIN Section s on s.Id = ssm.SectionId\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/studentsectionperformance')\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 4) Create spark db's that point to the data in the data lake to allow for connecting via Power BI through use of the Serverless SQL endpoint.\r\n",
							"contoso_sis.create_stage2_db('PARQUET')\r\n",
							"m365.create_stage2_db('PARQUET')\r\n",
							"\r\n",
							"spark.sql('CREATE DATABASE IF NOT EXISTS s2_ContosoISD')\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Activity using PARQUET location '\" + oea.stage2np + \"/m365/TechActivity'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Calendar using PARQUET location '\" + oea.stage2np + \"/m365/Calendar'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Org using PARQUET location '\" + oea.stage2np + \"/m365/Org'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Person using PARQUET location '\" + oea.stage2np + \"/m365/Person'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.PersonIdentifier using PARQUET location '\" + oea.stage2np + \"/m365/PersonIdentifier'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.RefDefinition using PARQUET location '\" + oea.stage2np + \"/m365/RefDefinition'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Section using PARQUET location '\" + oea.stage2np + \"/m365/Section'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Session using PARQUET location '\" + oea.stage2np + \"/m365/Session'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StaffOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StaffOrgAffiliation'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StaffSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StaffSectionMembership'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StudentOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StudentOrgAffiliation'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StudentSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StudentSectionMembership'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Course using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Course'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Attendance using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Attendance'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.SectionMark using PARQUET location '\" + oea.stage2np + \"/ContosoISD/SectionMark'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Demographics using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Demographics'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StudentSectionPerformance using PARQUET location '\" + oea.stage2np + \"/ContosoISD/studentsectionperformance'\")\r\n",
							"\r\n",
							"print(f\"Created spark db's.\\nCurrent datasource: {oea.serverless_sql_endpoint}\")"
						],
						"outputs": [],
						"execution_count": 23
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from notebookutils import mssparkutils\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType, ShortType\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"import logging\r\n",
							"import pandas as pd\r\n",
							"import sys\r\n",
							"import re\r\n",
							"import json\r\n",
							"import datetime\r\n",
							"import random\r\n",
							"import io\r\n",
							"\r\n",
							"logger = logging.getLogger('OEA')\r\n",
							"\r\n",
							"class OEA:\r\n",
							"    def __init__(self, storage_account='', instrumentation_key='', salt='', logging_level=logging.DEBUG):\r\n",
							"        if storage_account:\r\n",
							"            self.storage_account = storage_account\r\n",
							"        else:\r\n",
							"            oea_id = mssparkutils.env.getWorkspaceName()[8:] # extracts the OEA id for this OEA instance from the synapse workspace name (based on OEA naming convention)\r\n",
							"            self.storage_account = 'stoea' + oea_id # sets the name of the storage account based on OEA naming convention\r\n",
							"        self.serverless_sql_endpoint = mssparkutils.env.getWorkspaceName() + '-ondemand.sql.azuresynapse.net'\r\n",
							"        self._initialize_logger(instrumentation_key, logging_level)\r\n",
							"        self.salt = salt\r\n",
							"        self.stage1np = 'abfss://stage1np@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"        self.stage2np = 'abfss://stage2np@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"        self.stage2p = 'abfss://stage2p@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"        self.stage3np = 'abfss://stage3np@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"        self.stage3p = 'abfss://stage3p@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"        self.framework_path = 'abfss://oea-framework@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"\r\n",
							"        logger.debug(\"OEA initialized.\")\r\n",
							"\r\n",
							"    def _initialize_logger(self, instrumentation_key, logging_level):\r\n",
							"        logging.lastResort = None\r\n",
							"        # the logger will print an error like \"ValueError: I/O operation on closed file\" because we're trying to have log messages also print to stdout\r\n",
							"        # and apparently this causes issues on some of the spark executor nodes. The bottom line is that we don't want these logging errors to get printed in the notebook output.\r\n",
							"        logging.raiseExceptions = False\r\n",
							"        logger.setLevel(logging_level)\r\n",
							"\r\n",
							"        handler = logging.StreamHandler(sys.stdout)\r\n",
							"        handler.setLevel(logging_level)\r\n",
							"        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\r\n",
							"        handler.setFormatter(formatter)\r\n",
							"        logger.addHandler(handler) \r\n",
							"\r\n",
							"    def load(self, folder, table, stage=None, data_format='delta'):\r\n",
							"        \"\"\" Loads a dataframe based on the path specified in the given args \"\"\"\r\n",
							"        if stage is None: stage = self.stage2p\r\n",
							"        path = f\"{stage}/{folder}/{table}\"\r\n",
							"        try:\r\n",
							"            df = spark.read.load(f\"{stage}/{folder}/{table}\", format=data_format)\r\n",
							"            return df        \r\n",
							"        except AnalysisException as e:\r\n",
							"            raise ValueError(\"Failed to load. Are you sure you have the right path?\\nMore info below:\\n\" + str(e))\r\n",
							"\r\n",
							"    def load_from_stage1(self, path_and_filename, data_format='csv'):\r\n",
							"        \"\"\" Loads a dataframe with data from stage1, based on the path specified in the given args \"\"\"\r\n",
							"        path = f\"{self.stage1np}/{path_and_filename}\"\r\n",
							"        df = spark.read.load(path, format=data_format)\r\n",
							"        return df        \r\n",
							"\r\n",
							"    def load_sample_from_csv_file(self, path_and_filename, header=True, stage=None):\r\n",
							"        \"\"\" Loads a sample from the specified csv file and returns a pandas dataframe.\r\n",
							"            Ex: print(load_sample_from_csv_file('/student_data/students.csv'))\r\n",
							"        \"\"\"\r\n",
							"        if stage is None: stage = self.stage1np\r\n",
							"        csv_str = mssparkutils.fs.head(f\"{stage}/{path_and_filename}\") # https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python#preview-file-content\r\n",
							"        complete_lines = re.match(r\".*\\n\", csv_str, re.DOTALL).group(0)\r\n",
							"        if header: header = 0 # for info on why this is needed: https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.read_csv.html\r\n",
							"        else: header = None\r\n",
							"        pdf = pd.read_csv(io.StringIO(complete_lines), sep=',', header=header)\r\n",
							"        return pdf\r\n",
							"\r\n",
							"    def print_stage(self, path):\r\n",
							"        \"\"\" Prints out the highlevel contents of the specified stage.\"\"\"\r\n",
							"        msg = path + \"\\n\"\r\n",
							"        folders = self.get_folders(path)\r\n",
							"        for folder_name in folders:\r\n",
							"            entities = self.get_folders(path + '/' + folder_name)\r\n",
							"            msg += f\"{folder_name}: {entities}\\n\"\r\n",
							"        print(msg)            \r\n",
							"\r\n",
							"    def fix_column_names(self, df):\r\n",
							"        \"\"\" Fix column names to satisfy the Parquet naming requirements by substituting invalid characters with an underscore. \"\"\"\r\n",
							"        df_with_valid_column_names = df.select([F.col(col).alias(re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", col)) for col in df.columns])\r\n",
							"        return df_with_valid_column_names\r\n",
							"\r\n",
							"    def to_spark_schema(self, schema):#: list[list[str]]):\r\n",
							"        \"\"\" Creates a spark schema from a schema specified in the OEA schema format. \r\n",
							"            Example:\r\n",
							"            schemas['Person'] = [['Id','string','hash'],\r\n",
							"                                    ['CreateDate','timestamp','no-op'],\r\n",
							"                                    ['LastModifiedDate','timestamp','no-op']]\r\n",
							"            to_spark_schema(schemas['Person'])\r\n",
							"        \"\"\"\r\n",
							"        fields = []\r\n",
							"        for col_name, dtype, op in schema:\r\n",
							"            fields.append(StructField(col_name, globals()[dtype.lower().capitalize() + \"Type\"](), True))\r\n",
							"        spark_schema = StructType(fields)\r\n",
							"        return spark_schema\r\n",
							"\r\n",
							"    def pseudonymize(self, df, schema): #: list[list[str]]):\r\n",
							"        \"\"\" Performs pseudonymization of the given dataframe based on the provided schema.\r\n",
							"            For example, if the given df is for an entity called person, \r\n",
							"            2 dataframes will be returned, one called person that has hashed ids and masked fields, \r\n",
							"            and one called person_lookup that contains the original person_id, person_id_pseudo,\r\n",
							"            and the non-masked values for columns marked to be masked.\"\"\"\r\n",
							"        \r\n",
							"        df_pseudo = df_lookup = df\r\n",
							"\r\n",
							"        for col_name, dtype, op in schema:\r\n",
							"            if op == \"hash-no-lookup\" or op == \"hnl\":\r\n",
							"                # This means that the lookup can be performed against a different table so no lookup is needed.\r\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
							"                df_lookup = df_lookup.drop(col_name)           \r\n",
							"            elif op == \"hash\" or op == 'h':\r\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
							"                df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256))\r\n",
							"            elif op == \"mask\" or op == 'm':\r\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\r\n",
							"            elif op == \"no-op\" or op == 'x':\r\n",
							"                df_lookup = df_lookup.drop(col_name)\r\n",
							"\r\n",
							"        df_pseudo = self.fix_column_names(df_pseudo)\r\n",
							"        df_lookup = self.fix_column_names(df_lookup)\r\n",
							"\r\n",
							"        return (df_pseudo, df_lookup)\r\n",
							"\r\n",
							"    # Returns true if the path exists\r\n",
							"    def path_exists(self, path):\r\n",
							"        tableExists = False\r\n",
							"        try:\r\n",
							"            items = mssparkutils.fs.ls(path)\r\n",
							"            tableExists = True\r\n",
							"        except Exception as e:\r\n",
							"            # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\r\n",
							"            pass\r\n",
							"        return tableExists\r\n",
							"\r\n",
							"    def ls(self, path):\r\n",
							"        folders = []\r\n",
							"        files = []\r\n",
							"        try:\r\n",
							"            items = mssparkutils.fs.ls(path)\r\n",
							"            for item in items:\r\n",
							"                if item.isFile:\r\n",
							"                    files.append(item.name)\r\n",
							"                elif item.isDir:\r\n",
							"                    folders.append(item.name)\r\n",
							"        except Exception as e:\r\n",
							"            logger.warning(\"[OEA] Could not peform ls on specified path: \" + path + \"\\nThis may be because the path does not exist.\")\r\n",
							"        return (folders, files)\r\n",
							"\r\n",
							"    def print_stage(self, path):\r\n",
							"        print(path)\r\n",
							"        folders = self.get_folders(path)\r\n",
							"        for folder_name in folders:\r\n",
							"            entities = self.get_folders(path + '/' + folder_name)\r\n",
							"            print(f\"{folder_name}: {entities}\")\r\n",
							"\r\n",
							"    # Return the list of folders found in the given path.\r\n",
							"    def get_folders(self, path):\r\n",
							"        dirs = []\r\n",
							"        try:\r\n",
							"            items = mssparkutils.fs.ls(path)\r\n",
							"            for item in items:\r\n",
							"                #print(item.name, item.isDir, item.isFile, item.path, item.size)\r\n",
							"                if item.isDir:\r\n",
							"                    dirs.append(item.name)\r\n",
							"        except Exception as e:\r\n",
							"            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\r\n",
							"        return dirs\r\n",
							"\r\n",
							"    # Remove a folder if it exists (defaults to use of recursive removal).\r\n",
							"    def rm_if_exists(self, path, recursive_remove=True):\r\n",
							"        try:\r\n",
							"            mssparkutils.fs.rm(path, recursive_remove)\r\n",
							"        except Exception as e:\r\n",
							"            pass\r\n",
							"\r\n",
							"    def pop_from_path(self, path):\r\n",
							"        \"\"\" Pops the last arg in a path and returns the path and the last arg as a tuple.\r\n",
							"            pop_from_path('abfss://stage2@xyz.dfs.core.windows.net/ms_insights/test.csv') # returns ('abfss://stage2@xyz.dfs.core.windows.net/ms_insights', 'test.csv')\r\n",
							"        \"\"\"\r\n",
							"        m = re.match(r\"(.*)\\/([^/]+)\", path)\r\n",
							"        return (m.group(1), m.group(2))\r\n",
							"\r\n",
							"    def parse_source_path(self, path):\r\n",
							"        \"\"\" Parses a path that looks like this: abfss://stage2@stoeacisd3ggimpl3.dfs.core.windows.net/ms_insights\r\n",
							"            and returns a dictionary like this: {'stage_num': '2', 'ss': 'ms_insights'}\r\n",
							"            Note that it will also return a 'stage_num' of 2 if the path is stage2p - this is by design because the spark db with the s2 prefix will be used for data in stage2 and stage2p.\r\n",
							"        \"\"\"\r\n",
							"        m = re.match(r\".*:\\/\\/stage(?P<stage_num>\\d+)[n]?[p]?@[^/]+\\/(?P<ss>[^/]+)\", path)\r\n",
							"        return m.groupdict()\r\n",
							"    \r\n",
							"    def create_db(self, source_path, source_format='DELTA'):\r\n",
							"        \"\"\" Creates a spark db based on the given path (assumes that every folder in the given path is a table).\r\n",
							"            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\r\n",
							"        \"\"\"\r\n",
							"        source_info = self.parse_source_path(source_path)\r\n",
							"        db_name = f\"s{source_info['stage_num']}_{source_info['ss']}\"\r\n",
							"        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\r\n",
							"        dirs = self.get_folders(source_path)\r\n",
							"        for table_name in dirs:\r\n",
							"            spark.sql(f\"create table if not exists {db_name}.{table_name} using {source_format} location '{source_path}/{table_name}'\")\r\n",
							"        result = \"Database created: \" + db_name\r\n",
							"        logger.info(result)\r\n",
							"        return result\r\n",
							"\r\n",
							"    def drop_db(self, db_name):\r\n",
							"        \"\"\" Drop all tables in a db, then drop the db. \"\"\"\r\n",
							"        df = spark.sql('SHOW TABLES FROM ' + db_name)\r\n",
							"        for row in df.rdd.collect():\r\n",
							"            spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{row['tableName']}\")\r\n",
							"        spark.sql(f\"DROP DATABASE {db_name}\")\r\n",
							"        result = \"Database dropped: \" + db_name\r\n",
							"        logger.info(result)\r\n",
							"        return result         \r\n",
							"\r\n",
							"    # List installed packages\r\n",
							"    def list_packages(self):\r\n",
							"        import pkg_resources\r\n",
							"        for d in pkg_resources.working_set:\r\n",
							"            print(d)\r\n",
							"\r\n",
							"    def print_schema_starter(self, entity_name, df):\r\n",
							"        \"\"\" Prints a starter schema that can be modified as needed when developing the oea schema for a new module. \"\"\"\r\n",
							"        st = f\"self.schemas['{entity_name}'] = [\"\r\n",
							"        for col in df.schema:\r\n",
							"            st += f\"['{col.name}', '{str(col.dataType)[:-4].lower()}', 'no-op'],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\"\r\n",
							"        return st[:-11] + ']'\r\n",
							"\r\n",
							"    def write_dict_as_csv(data, folder, filename, container=None):\r\n",
							"        \"\"\" Writes a dictionary as a csv to the specified location. This is helpful when creating test data sets and landing them in stage1np. \"\"\"\r\n",
							"        if container == None: container = self.stage1np\r\n",
							"        pdf = pd.DataFrame(data)\r\n",
							"        mssparkutils.fs.put(f\"{container}/{folder}/{filename}\", pdf.to_csv(index=False), True) # True indicates overwrite mode  \r\n",
							"\r\n",
							"class BaseOEAModule:\r\n",
							"    \"\"\" Provides data processing methods for Contoso SIS data (the student information system for the fictional Contoso school district).  \"\"\"\r\n",
							"    def __init__(self, oea, source_folder, pseudonymize = True):\r\n",
							"        self.pseudonymize = pseudonymize\r\n",
							"        self.oea = oea\r\n",
							"        self.stage1np = f\"{oea.stage1np}/{source_folder}\"\r\n",
							"        self.stage2np = f\"{oea.stage2np}/{source_folder}\"\r\n",
							"        self.stage2p = f\"{oea.stage2p}/{source_folder}\"\r\n",
							"        self.stage3np = f\"{oea.stage3np}/{source_folder}\"\r\n",
							"        self.stage3p = f\"{oea.stage3p}/{source_folder}\"\r\n",
							"        self.module_path = f\"{oea.framework_path}/modules/{source_folder}\"\r\n",
							"        self.schemas = {}\r\n",
							"   \r\n",
							"    def _process_entity_from_stage1(self, entity_name, format='csv', write_mode='overwrite', header='true'):\r\n",
							"        spark_schema = self.oea.to_spark_schema(self.schemas[entity_name])\r\n",
							"        df = spark.read.format(format).load(f\"{self.stage1np}/{entity_name}\", header=header, schema=spark_schema)\r\n",
							"\r\n",
							"        if self.pseudonymize:\r\n",
							"            df_pseudo, df_lookup = self.oea.pseudonymize(df, self.schemas[entity_name])\r\n",
							"            df_pseudo.write.format('delta').mode(write_mode).save(f\"{self.stage2p}/{entity_name}\")\r\n",
							"            if len(df_lookup.columns) > 0:\r\n",
							"                df_lookup.write.format('delta').mode(write_mode).save(f\"{self.stage2np}/{entity_name}_lookup\")\r\n",
							"        else:\r\n",
							"            df = self.oea.fix_column_names(df)   \r\n",
							"            df.write.format('delta').mode(write_mode).save(f\"{self.stage2np}/{entity_name}\")\r\n",
							"\r\n",
							"    def delete_stage1(self):\r\n",
							"        self.oea.rm_if_exists(self.stage1np)\r\n",
							"\r\n",
							"    def delete_stage2(self):\r\n",
							"        self.oea.rm_if_exists(self.stage2np)\r\n",
							"        self.oea.rm_if_exists(self.stage2p)\r\n",
							"\r\n",
							"    def delete_stage3(self):\r\n",
							"        self.oea.rm_if_exists(self.stage3np)\r\n",
							"        self.oea.rm_if_exists(self.stage3p)                \r\n",
							"\r\n",
							"    def delete_all_stages(self):\r\n",
							"        self.delete_stage1()\r\n",
							"        self.delete_stage2()\r\n",
							"        self.delete_stage3()\r\n",
							"\r\n",
							"    def create_stage2_db(self, format='DELTA'):\r\n",
							"        self.oea.create_db(self.stage2p, format)\r\n",
							"        self.oea.create_db(self.stage2np, format)\r\n",
							"\r\n",
							"    def create_stage3_db(self, format='DELTA'):\r\n",
							"        self.oea.create_db(self.stage3p, format)\r\n",
							"        self.oea.create_db(self.stage3np, format)\r\n",
							"\r\n",
							"    def copy_test_data_to_stage1(self):\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data', self.stage1np, True)   \r\n",
							"\r\n",
							"class DataLakeWriter:\r\n",
							"    def __init__(self, root_destination):\r\n",
							"        self.root_destination = root_destination\r\n",
							"\r\n",
							"    def write(self, path_and_filename, data_str, format='csv'):\r\n",
							"        mssparkutils.fs.append(f\"{self.root_destination}/{path_and_filename}\", data_str, True) # Set the last parameter as True to create the file if it does not exist"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/contosa_module_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7628ba38-9062-4f02-9336-f8da6e5d6867"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/88bc4ad2-b92a-4cec-9c63-bac0d9696a30/resourceGroups/rg-oea-eshthon21/providers/Microsoft.Synapse/workspaces/syn-oea-eshthon21/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://syn-oea-eshthon21.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"class ContosoSIS(BaseOEAModule):\r\n",
							"    def __init__(self, oea, source_folder='contoso_sis', pseudonymize = True):\r\n",
							"        BaseOEAModule.__init__(self, oea, source_folder, pseudonymize)\r\n",
							"        self.schemas['studentattendance'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['school_year', 'integer', 'no-op'],\r\n",
							"                                            ['school_id', 'string', 'no-op'],\r\n",
							"                                            ['attendance_date', 'timestamp', 'no-op'],\r\n",
							"                                            ['all_day', 'string', 'no-op'],\r\n",
							"                                            ['Period', 'short', 'no-op'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['AttendanceCode', 'string', 'no-op'],\r\n",
							"                                            ['PresenceFlag', 'boolean', 'no-op'],\r\n",
							"                                            ['attendance_status', 'string', 'no-op'],\r\n",
							"                                            ['attendance_type', 'string', 'no-op'],\r\n",
							"                                            ['attendance_sequence', 'short', 'no-op']]\r\n",
							"\r\n",
							"        self.schemas['studentsectionmark'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['school_year', 'string', 'no-op'],\r\n",
							"                                            ['term_id', 'string', 'no-op'],\r\n",
							"                                            ['numeric_grade_earned', 'short', 'no-op'],\r\n",
							"                                            ['alpha_grade_earned', 'string', 'no-op'],\r\n",
							"                                            ['is_final_grade', 'string', 'no-op'],\r\n",
							"                                            ['credits_attempted', 'short', 'no-op'],\r\n",
							"                                            ['credits_earned', 'short', 'no-op'],\r\n",
							"                                            ['grad_credit_type', 'string', 'no-op']]\r\n",
							"\r\n",
							"        self.schemas['studentdemographics'] = [['SIS ID', 'string', 'no-op'],\r\n",
							"                                            ['FederalRaceCategory', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['PrimaryLanguage', 'string', 'no-op'],\r\n",
							"                                            ['ELLStatus', 'string', 'no-op'],\r\n",
							"                                            ['SpecialEducation', 'string', 'no-op'],\r\n",
							"                                            ['LowIncome', 'boolean', 'no-op']]\r\n",
							"    def process_data_from_stage1(self):\r\n",
							"        self._process_entity_from_stage1('studentattendance', 'csv', 'overwrite', 'true')\r\n",
							"        self._process_entity_from_stage1('studentsectionmark', 'csv', 'overwrite', 'true')\r\n",
							"        self._process_entity_from_stage1('studentdemographics', 'csv', 'overwrite', 'true')\r\n",
							"\r\n",
							"    def copy_test_data_to_stage1(self):\r\n",
							"        #Modifiying test data for visualization purposes\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentattendancemodified.csv', self.stage1np + '/studentattendance/studentattendance.csv', True)\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentsectionmark.csv', self.stage1np + '/studentsectionmark/studentsectionmark.csv', True)\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentdemographics.csv', self.stage1np + '/studentdemographics/studentdemographics.csv', True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/m365_module_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "95c1f629-0042-41ff-ab32-a6705bfa9fd1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/88bc4ad2-b92a-4cec-9c63-bac0d9696a30/resourceGroups/rg-oea-eshthon21/providers/Microsoft.Synapse/workspaces/syn-oea-eshthon21/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://syn-oea-eshthon21.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"class ContosoSIS(BaseOEAModule):\r\n",
							"    def __init__(self, oea, source_folder='contoso_sis', pseudonymize = True):\r\n",
							"        BaseOEAModule.__init__(self, oea, source_folder, pseudonymize)\r\n",
							"        self.schemas['studentattendance'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['school_year', 'integer', 'no-op'],\r\n",
							"                                            ['school_id', 'string', 'no-op'],\r\n",
							"                                            ['attendance_date', 'timestamp', 'no-op'],\r\n",
							"                                            ['all_day', 'string', 'no-op'],\r\n",
							"                                            ['Period', 'short', 'no-op'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['AttendanceCode', 'string', 'no-op'],\r\n",
							"                                            ['PresenceFlag', 'boolean', 'no-op'],\r\n",
							"                                            ['attendance_status', 'string', 'no-op'],\r\n",
							"                                            ['attendance_type', 'string', 'no-op'],\r\n",
							"                                            ['attendance_sequence', 'short', 'no-op']]\r\n",
							"\r\n",
							"        self.schemas['studentsectionmark'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['school_year', 'string', 'no-op'],\r\n",
							"                                            ['term_id', 'string', 'no-op'],\r\n",
							"                                            ['numeric_grade_earned', 'short', 'no-op'],\r\n",
							"                                            ['alpha_grade_earned', 'string', 'no-op'],\r\n",
							"                                            ['is_final_grade', 'string', 'no-op'],\r\n",
							"                                            ['credits_attempted', 'short', 'no-op'],\r\n",
							"                                            ['credits_earned', 'short', 'no-op'],\r\n",
							"                                            ['grad_credit_type', 'string', 'no-op']]\r\n",
							"\r\n",
							"        self.schemas['studentdemographics'] = [['SIS ID', 'string', 'no-op'],\r\n",
							"                                            ['FederalRaceCategory', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['PrimaryLanguage', 'string', 'no-op'],\r\n",
							"                                            ['ELLStatus', 'string', 'no-op'],\r\n",
							"                                            ['SpecialEducation', 'string', 'no-op'],\r\n",
							"                                            ['LowIncome', 'boolean', 'no-op']]\r\n",
							"    def process_data_from_stage1(self):\r\n",
							"        self._process_entity_from_stage1('studentattendance', 'csv', 'overwrite', 'true')\r\n",
							"        self._process_entity_from_stage1('studentsectionmark', 'csv', 'overwrite', 'true')\r\n",
							"        self._process_entity_from_stage1('studentdemographics', 'csv', 'overwrite', 'true')\r\n",
							"\r\n",
							"    def copy_test_data_to_stage1(self):\r\n",
							"        #Modifiying test data for visualization purposes\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentattendancemodified.csv', self.stage1np + '/studentattendance/studentattendance.csv', True)\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentsectionmark.csv', self.stage1np + '/studentsectionmark/studentsectionmark.csv', True)\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentdemographics.csv', self.stage1np + '/studentdemographics/studentdemographics.csv', True)\r\n",
							"\r\n",
							"class M365(BaseOEAModule):\r\n",
							"    \"\"\"\r\n",
							"    Provides data processing methods for MS Insights data v0.2 format.\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    def __init__(self, oea, source_folder='m365'):\r\n",
							"        BaseOEAModule.__init__(self, oea, source_folder)\r\n",
							"\r\n",
							"        self.stage1np_activity = self.stage1np + '/DIPData/Activity/ApplicationUsage'\r\n",
							"        self.stage1np_roster = self.stage1np + '/DIPData/Roster'\r\n",
							"\r\n",
							"        self.schemas['Activity0p2'] = [['SignalType', 'string', 'no-op'],\r\n",
							"                                            ['StartTime', 'timestamp', 'no-op'],\r\n",
							"                                            ['UserAgent', 'string', 'no-op'],\r\n",
							"                                            ['SignalId', 'string', 'no-op'],\r\n",
							"                                            ['SISClassId', 'string', 'no-op'],\r\n",
							"                                            ['OfficeClassId', 'string', 'no-op'],\r\n",
							"                                            ['ChannelId', 'string', 'no-op'],\r\n",
							"                                            ['AppName', 'string', 'no-op'],\r\n",
							"                                            ['ActorId', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['ActorRole', 'string', 'no-op'],\r\n",
							"                                            ['SchemaVersion', 'string', 'no-op'],\r\n",
							"                                            ['AssignmentId', 'string', 'no-op'],\r\n",
							"                                            ['SubmissionId', 'string', 'no-op'],\r\n",
							"                                            ['Action', 'string', 'no-op'],\r\n",
							"                                            ['AssginmentDueDate', 'string', 'no-op'],\r\n",
							"                                            ['ClassCreationDate', 'string', 'no-op'],\r\n",
							"                                            ['Grade', 'string', 'no-op'],\r\n",
							"                                            ['SourceFileExtension', 'string', 'no-op'],\r\n",
							"                                            ['MeetingDuration', 'string', 'no-op']]\r\n",
							"        self.schemas['Calendar'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['SchoolYear', 'integer', 'no-op'],\r\n",
							"                                            ['IsCurrent', 'boolean', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op']]\r\n",
							"        self.schemas['Course'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Code', 'string', 'no-op'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['CalendarId', 'string', 'no-op']]\r\n",
							"        self.schemas['Org'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Identifier', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['ParentOrgId', 'string', 'no-op'],\r\n",
							"                                            ['RefOrgTypeId', 'string', 'no-op'],\r\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\r\n",
							"        self.schemas['Person'] = [['Id', 'string', 'hash'],\r\n",
							"                                            ['FirstName', 'string', 'mask'],\r\n",
							"                                            ['MiddleName', 'string', 'mask'],\r\n",
							"                                            ['LastName', 'string', 'mask'],\r\n",
							"                                            ['GenerationCode', 'string', 'no-op'],\r\n",
							"                                            ['Prefix', 'string', 'no-op'],\r\n",
							"                                            ['EnabledUser', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'hash'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\r\n",
							"        self.schemas['PersonIdentifier'] = [['Id', 'string', 'hash'],\r\n",
							"                                            ['Identifier', 'string', 'hash'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['RefIdentifierTypeId', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'hash'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\r\n",
							"        self.schemas['RefDefinition'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['RefType', 'string', 'no-op'],\r\n",
							"                                            ['Namespace', 'string', 'no-op'],\r\n",
							"                                            ['Code', 'string', 'no-op'],\r\n",
							"                                            ['SortOrder', 'integer', 'no-op'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op']]\r\n",
							"        self.schemas['Section'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Code', 'string', 'no-op'],\r\n",
							"                                            ['Location', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['CourseId', 'string', 'no-op'],\r\n",
							"                                            ['RefSectionTypeId', 'string', 'no-op'],\r\n",
							"                                            ['SessionId', 'string', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op']]\r\n",
							"        self.schemas['Session'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['BeginDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['EndDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['CalendarId', 'string', 'no-op'],\r\n",
							"                                            ['ParentSessionId', 'string', 'no-op'],\r\n",
							"                                            ['RefSessionTypeId', 'string', 'no-op']]\r\n",
							"        self.schemas['StaffOrgAffiliation'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['IsPrimary', 'boolean', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefStaffOrgRoleId', 'string', 'no-op']]\r\n",
							"        self.schemas['StaffSectionMembership'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['IsPrimaryStaffForSection', 'boolean', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefStaffSectionRoleId', 'string', 'no-op'],\r\n",
							"                                            ['SectionId', 'string', 'no-op']]\r\n",
							"        self.schemas['StudentOrgAffiliation'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['IsPrimary', 'boolean', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefGradeLevelId', 'string', 'no-op'],\r\n",
							"                                            ['RefStudentOrgRoleId', 'string', 'no-op'],\r\n",
							"                                            ['RefEnrollmentStatusId', 'string', 'no-op']]\r\n",
							"        self.schemas['StudentSectionMembership'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefGradeLevelWhenCourseTakenId', 'string', 'no-op'],\r\n",
							"                                            ['RefStudentSectionRoleId', 'string', 'no-op'],\r\n",
							"                                            ['SectionId', 'string', 'no-op']]\r\n",
							"    \r\n",
							"    def process_activity_data_from_stage1(self):\r\n",
							"        \"\"\" Processes activity data from stage1 into stage2 using structured streaming. \r\n",
							"            https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\r\n",
							"        \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights activity data from: \" + self.stage1np_activity)\r\n",
							"\r\n",
							"        spark_schema = self.oea.to_spark_schema(self.schemas['Activity0p2'])\r\n",
							"        df = spark.read.csv(self.stage1np_activity + '/*.csv', header='false', schema=spark_schema) \r\n",
							"        sqlContext.registerDataFrameAsTable(df, 'Activity')\r\n",
							"        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(self.oea.stage2np + '/m365/PersonIdentifier'), 'PersonIdentifier')\r\n",
							"        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(self.oea.stage2np + '/m365/RefDefinition'), 'RefDefinition')\r\n",
							"\r\n",
							"        df = spark.sql( \r\n",
							"            \"select act.SignalType, act.StartTime, act.UserAgent, act.SignalId, act.SISClassId, act.OfficeClassId, act.ChannelId, \\\r\n",
							"            act.AppName, act.ActorId, act.ActorRole, act.SchemaVersion, act.AssignmentId, act.SubmissionId, act.Action, act.AssginmentDueDate, \\\r\n",
							"            act.ClassCreationDate, act.Grade, act.SourceFileExtension, act.MeetingDuration, pi.PersonId \\\r\n",
							"            from PersonIdentifier pi, RefDefinition rd, Activity act \\\r\n",
							"            where \\\r\n",
							"                pi.RefIdentifierTypeId = rd.Id \\\r\n",
							"                and rd.RefType = 'RefIdentifierType' \\\r\n",
							"                and rd.Code = 'ActiveDirectoryId' \\\r\n",
							"                and pi.Identifier = act.ActorId\")\r\n",
							"\r\n",
							"        df = df.dropDuplicates(['SignalId'])\r\n",
							"        df = df.withColumn('year', F.year(F.col('StartTime'))).withColumn('month', F.month(F.col('StartTime')))\r\n",
							"        df = self.oea.fix_column_names(df)\r\n",
							"        df.write.format('parquet').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/TechActivity')\r\n",
							"\r\n",
							"    def reset_activity_processing(self):\r\n",
							"        \"\"\" Resets all TechActivity processing. This is intended for use during initial testing - use with caution. \"\"\"\r\n",
							"        self.oea.rm_if_exists(self.stage2p + '/TechActivity')\r\n",
							"        self.oea.rm_if_exists(self.stage2np + '/TechActivity')\r\n",
							"        logger.info(f\"Deleted TechActivity from stage2\")  \r\n",
							"\r\n",
							"    def _process_roster_entity(self, path):\r\n",
							"        try:\r\n",
							"            base_path, filename = self.oea.pop_from_path(path)\r\n",
							"            entity = filename[:-4]\r\n",
							"            logger.debug(f\"Processing roster entity: path={path}, entity={entity}\")\r\n",
							"            spark_schema = self.oea.to_spark_schema(self.schemas[entity])\r\n",
							"            df = spark.read.csv(path, header='false', schema=spark_schema)\r\n",
							"            df = self.oea.fix_column_names(df)\r\n",
							"            df.write.format('parquet').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/' + entity)\r\n",
							"\r\n",
							"        except (AnalysisException) as error:\r\n",
							"            logger.exception(str(error))\r\n",
							"\r\n",
							"    def process_roster_data_from_stage1(self):\r\n",
							"        \"\"\" Processes all roster data in stage1 and writes out to stage2 and stage2p \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights roster data from: \" + self.stage1np)\r\n",
							"\r\n",
							"        items = mssparkutils.fs.ls(self.stage1np_roster)\r\n",
							"        #print(items)\r\n",
							"        for item in items:\r\n",
							"            if item.isFile:\r\n",
							"                self._process_roster_entity(item.path)\r\n",
							"\r\n",
							"    def reset_roster_processing(self):\r\n",
							"        \"\"\" Resets all stage1 to stage2 processing of roster data. \"\"\"\r\n",
							"        # cleanup stage2np\r\n",
							"        if self.oea.path_exists(self.stage2np):\r\n",
							"            # Delete roster tables (everything other than TechActivity)\r\n",
							"            items = mssparkutils.fs.ls(self.stage2np)\r\n",
							"            #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"            for item in items:\r\n",
							"                if item.name != 'TechActivity':\r\n",
							"                    mssparkutils.fs.rm(item.path, True)\r\n",
							"        # cleanup stage2p\r\n",
							"        if self.oea.path_exists(self.stage2p):\r\n",
							"            # Delete roster tables (everything other than TechActivity)\r\n",
							"            items = mssparkutils.fs.ls(self.stage2p)\r\n",
							"            #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"            for item in items:\r\n",
							"                if item.name != 'TechActivity':\r\n",
							"                    mssparkutils.fs.rm(item.path, True)    \r\n",
							"  \r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		}
	]
}