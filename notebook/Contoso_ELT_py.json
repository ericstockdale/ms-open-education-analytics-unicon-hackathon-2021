{
	"name": "Contoso_ELT_py",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark3p1sm",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "ce99dc6b-54c4-43c7-893c-4a35d593741c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/88bc4ad2-b92a-4cec-9c63-bac0d9696a30/resourceGroups/rg-oea-eshthon21/providers/Microsoft.Synapse/workspaces/syn-oea-eshthon21/bigDataPools/spark3p1sm",
				"name": "spark3p1sm",
				"type": "Spark",
				"endpoint": "https://syn-oea-eshthon21.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%run /OEA_py"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run /contosa_module_py"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run /m365_module_py"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 0) Initialize the OEA framework and modules needed.\r\n",
					"oea = OEA()\r\n",
					"m365 = M365(oea)\r\n",
					"contoso_sis = ContosoSIS(oea, 'contoso_sis', False)"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def reset_all_processing():\r\n",
					"    contoso_sis.delete_all_stages()\r\n",
					"    m365.delete_all_stages()\r\n",
					"    oea.rm_if_exists(oea.stage2np + '/ContosoISD')\r\n",
					"\r\n",
					"    oea.drop_db('s2_contoso_sis')\r\n",
					"    oea.drop_db('s2_contosoisd')\r\n",
					"    oea.drop_db('s2_m365')\r\n",
					"\r\n",
					"# Uncomment the following line and run this cell to reset everything if you want to walk through the process again.\r\n",
					"reset_all_processing()"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 1) Land data into stage1 of your data lake, from multiple source systems (this example copies in test data sets that came with the OEA installation).\r\n",
					"contoso_sis.copy_test_data_to_stage1()\r\n",
					"m365.copy_test_data_to_stage1()"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 2) Process the raw data (csv format) from stage1 into stage2 (adds schema details and writes out in parquet format).\r\n",
					"#    [Note: we're not performing pseudonymization in this example, so everything is written to container stage2np.]\r\n",
					"m365.process_roster_data_from_stage1()\r\n",
					"contoso_sis.process_data_from_stage1()\r\n",
					"m365.process_activity_data_from_stage1()"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 3) Run additional prep on the data to create a unified dataset that can be used in a Power BI report\r\n",
					"\r\n",
					"# Process sectionmark data. Convert id values to use the Person.Id and Section.Id values set in the m365 data.\r\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentsectionmark'), 'SectionMark')\r\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Person'), 'Person')\r\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Section'), 'Section')\r\n",
					"df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\r\n",
					"sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\r\n",
					"sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\r\n",
					"from SectionMark sm, Person p, Section s \\\r\n",
					"where sm.student_id = p.ExternalId \\\r\n",
					"and sm.section_id = s.ExternalId\")\r\n",
					"df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/SectionMark')\r\n",
					"\r\n",
					"# Repeat the above process, this time for student attendance\r\n",
					"# Convert id values to use the Person.Id, Org.Id and Section.Id values\r\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentattendance'), 'Attendance')\r\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Org'), 'Org')\r\n",
					"df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, to_date(att.attendance_date,'MM/dd/yyyy') AttendanceDate, \\\r\n",
					"att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\r\n",
					"att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\r\n",
					"from Attendance att, Org o, Person p, Section s \\\r\n",
					"where att.student_id = p.ExternalId \\\r\n",
					"and att.school_id = o.ExternalId \\\r\n",
					"and att.section_id = s.ExternalId\")\r\n",
					"df.write.format('parquet').mode('overwrite').save(oea.stage2np +'/ContosoISD/Attendance')\r\n",
					"\r\n",
					"# Repeat the above process, this time for student demographics\r\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentdemographics'), 'Demographics')\r\n",
					"\r\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Person'), 'Person')\r\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/StudentSectionMembership'), 'StudentSectionMembership')\r\n",
					"\r\n",
					"df = spark.sql(\"select p.Id PersonId, dem.FederalRaceCategory, dem.PrimaryLanguage, dem.ELLStatus, dem.SpecialEducation, dem.LowIncome \\\r\n",
					"from Demographics dem, Person p \\\r\n",
					"where dem.SIS_ID = p.ExternalId\")\r\n",
					"df.write.format('parquet').mode('overwrite').save(oea.stage2np +'/ContosoISD/Demographics')\r\n",
					"\r\n",
					"# Add 'Department' column to Course (hardcoded to \"Math\" for this Contoso example)\r\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Course'), 'Course')\r\n",
					"df = spark.sql(\"select Id, Name, Code, Description, ExternalId, CreateDate, LastModifiedDate, IsActive, CalendarId, 'Math' Department from Course\")\r\n",
					"df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/Course')\r\n",
					"\r\n",
					"\r\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/ContosoISD/SectionMark'), 'PersonSectionMark')\r\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/ContosoISD/Attendance'), 'PersonAttendance')\r\n",
					"\r\n",
					"df = spark.sql(\"select DISTINCT s.CourseId, s.Id SectionId, ssm.PersonId PersonId, \\\r\n",
					"CASE WHEN att.AttendanceCode = 'P' THEN 1 ELSE 0 END As WasPresent, \\\r\n",
					"sm.NumericGrade \\\r\n",
					"from StudentSectionMembership ssm \\\r\n",
					"LEFT JOIN PersonSectionMark sm on sm.SectionId = ssm.SectionId and sm.PersonId = ssm.PersonId \\\r\n",
					"LEFT JOIN PersonAttendance att on att.SectionId = ssm.SectionId and att.PersonId = ssm.PersonId \\\r\n",
					"JOIN Section s on s.Id = ssm.SectionId\")\r\n",
					"df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/studentsectionperformance')\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					""
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 4) Create spark db's that point to the data in the data lake to allow for connecting via Power BI through use of the Serverless SQL endpoint.\r\n",
					"contoso_sis.create_stage2_db('PARQUET')\r\n",
					"m365.create_stage2_db('PARQUET')\r\n",
					"\r\n",
					"spark.sql('CREATE DATABASE IF NOT EXISTS s2_ContosoISD')\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Activity using PARQUET location '\" + oea.stage2np + \"/m365/TechActivity'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Calendar using PARQUET location '\" + oea.stage2np + \"/m365/Calendar'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Org using PARQUET location '\" + oea.stage2np + \"/m365/Org'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Person using PARQUET location '\" + oea.stage2np + \"/m365/Person'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.PersonIdentifier using PARQUET location '\" + oea.stage2np + \"/m365/PersonIdentifier'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.RefDefinition using PARQUET location '\" + oea.stage2np + \"/m365/RefDefinition'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Section using PARQUET location '\" + oea.stage2np + \"/m365/Section'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Session using PARQUET location '\" + oea.stage2np + \"/m365/Session'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.StaffOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StaffOrgAffiliation'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.StaffSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StaffSectionMembership'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.StudentOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StudentOrgAffiliation'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.StudentSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StudentSectionMembership'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Course using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Course'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Attendance using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Attendance'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.SectionMark using PARQUET location '\" + oea.stage2np + \"/ContosoISD/SectionMark'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Demographics using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Demographics'\")\r\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.StudentSectionPerformance using PARQUET location '\" + oea.stage2np + \"/ContosoISD/studentsectionperformance'\")\r\n",
					"\r\n",
					"print(f\"Created spark db's.\\nCurrent datasource: {oea.serverless_sql_endpoint}\")"
				],
				"execution_count": 23
			}
		]
	}
}